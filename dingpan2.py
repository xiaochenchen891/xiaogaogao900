# app.py
import streamlit as st
import pandas as pd
import numpy as np
import time
from datetime import datetime, timedelta
import plotly.graph_objects as go
import plotly.express as px
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import urllib.parse
import os
import tempfile
import warnings
from scipy import stats
import logging
import shutil
import io
import re
import calendar
from dateutil.relativedelta import relativedelta
warnings.filterwarnings('ignore')

# è®¾ç½® logging é…ç½®
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# ====================== é¡µé¢é…ç½® ======================
st.set_page_config(
    page_title="åŒèŠ±é¡ºé—®è´¢ç›‘æ§ç³»ç»Ÿ",
    page_icon="ğŸ“ˆ",
    layout="wide"
)
st.title("åŒèŠ±ç›‘æ§ç³»ç»Ÿ")
st.markdown("---")

# ====================== StockMonitor ç±» ======================
class StockMonitor:
    def __init__(self):
        self.driver = None
        self.download_dir = tempfile.mkdtemp()
        self.profile_dir = tempfile.mkdtemp()
        # å›ºåŒ–åŒ¹é…ç¼“å­˜
        self.cached_selectors = {
            'search_box': {
                'selector': "//textarea[contains(@placeholder,'è¯·è¾“å…¥')]",
                'description': "æœç´¢æ¡† - è¯·è¾“å…¥æ¦‚å¿µã€ä»·å‡é‡ç¼©ç­‰ï¼Œå¤šä¸ªæ¡",
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            'search_button': {
                'selector': "//*[contains(@class,'search-icon')]",
                'description': "æœç´¢æŒ‰é’® - æ— æ–‡æœ¬",
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            'download_button': None
        }
        # ç›‘æ§æ•°æ®å­˜å‚¨
        self.monitoring_data = {
            'timestamps': [],
            'stock_counts': [],
            'stock_lists': [],
            'slope_data': [],
            'closing_sequences': [],
            'date_columns': [],
            'stock_names': [],
            'new_stocks': []
        }
        # ç›‘æ§çŠ¶æ€
        self.is_monitoring = False
        self.last_execution_time = None
        self.next_execution_time = None
        self.monitoring_interval = 5
        self.cycle_count = 1
        # å»¶è¿Ÿåˆå§‹åŒ–æµè§ˆå™¨
        self.driver_initialized = False
        # ç™»å½•çŠ¶æ€
        self.is_logged_in = False
        self.login_attempted = False
        # ä¸‹è½½å†å²
        self.last_download_time = None
        self.downloaded_files_history = []
        # å€’è®¡æ—¶
        self.countdown_seconds = 0

    # ==================== ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç†æµè§ˆå™¨é©±åŠ¨ ====================
    def initialize_driver(self):
        if self.driver_initialized and self.driver:
            logging.debug("æ­¥éª¤: Driver already initialized.")
            return True
        
        try:
            return self.initialize_chrome_with_manager()
        except Exception as e:
            logging.error(f"Chrome initialization failed: {str(e)}")
            try:
                return self.initialize_edge_with_manager()
            except Exception as e2:
                logging.error(f"Edge initialization also failed: {str(e2)}")
                st.error(f"æ‰€æœ‰æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ã€‚é”™è¯¯: {str(e)}")
                return False

    def initialize_chrome_with_manager(self):
        """ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç† Chrome é©±åŠ¨"""
        try:
            logging.debug("æ­¥éª¤: Initializing Chrome with webdriver-manager...")
            
            from selenium.webdriver.chrome.options import Options as ChromeOptions
            from selenium.webdriver.chrome.service import Service as ChromeService
            from webdriver_manager.chrome import ChromeDriverManager
            
            chrome_options = ChromeOptions()
            
            # åŸºæœ¬é…ç½®
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ç”¨æˆ·æ•°æ®ç›®å½•é…ç½®
            if self.profile_dir:
                chrome_options.add_argument(f'--user-data-dir={self.profile_dir}')
            
            # æ€§èƒ½ä¼˜åŒ–å‚æ•°
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            
            # ä¸‹è½½é…ç½®
            prefs = {
                "download.default_directory": self.download_dir,
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": False,
                "profile.default_content_settings.popups": 0,
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            # ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ä¸‹è½½å’Œç®¡ç† ChromeDriver
            service = ChromeService(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            self.driver.maximize_window()
            self.driver.implicitly_wait(5)
            self.driver_initialized = True
            logging.debug("æ­¥éª¤: Chrome driver initialized successfully with webdriver-manager.")
            st.success("âœ… å·²æˆåŠŸä½¿ç”¨ Chrome æµè§ˆå™¨")
            return True
            
        except Exception as e:
            logging.error(f"Error initializing Chrome with webdriver-manager: {str(e)}")
            raise e

    def initialize_edge_with_manager(self):
        """ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç† Edge é©±åŠ¨"""
        try:
            logging.debug("æ­¥éª¤: Initializing Edge with webdriver-manager...")
            
            from selenium.webdriver.edge.options import Options as EdgeOptions
            from selenium.webdriver.edge.service import Service as EdgeService
            from webdriver_manager.microsoft import EdgeChromiumDriverManager
            
            edge_options = EdgeOptions()
            
            # åŸºæœ¬é…ç½®
            edge_options.add_argument('--no-sandbox')
            edge_options.add_argument('--disable-dev-shm-usage')
            edge_options.add_argument('--disable-blink-features=AutomationControlled')
            edge_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            edge_options.add_experimental_option('useAutomationExtension', False)
            
            # ç”¨æˆ·æ•°æ®ç›®å½•é…ç½®
            if self.profile_dir:
                edge_options.add_argument(f'--user-data-dir={self.profile_dir}')
            
            # æ€§èƒ½ä¼˜åŒ–å‚æ•°
            edge_options.add_argument('--disable-gpu')
            edge_options.add_argument('--disable-extensions')
            edge_options.add_argument('--disable-plugins')
            
            # ä¸‹è½½é…ç½®
            prefs = {
                "download.default_directory": self.download_dir,
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": False,
                "profile.default_content_settings.popups": 0,
            }
            edge_options.add_experimental_option("prefs", prefs)
            
            # ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ä¸‹è½½å’Œç®¡ç† EdgeDriver
            service = EdgeService(EdgeChromiumDriverManager().install())
            self.driver = webdriver.Edge(service=service, options=edge_options)
            
            self.driver.maximize_window()
            self.driver.implicitly_wait(5)
            self.driver_initialized = True
            logging.debug("æ­¥éª¤: Edge driver initialized successfully with webdriver-manager.")
            st.success("âœ… å·²æˆåŠŸä½¿ç”¨ Edge æµè§ˆå™¨")
            return True
            
        except Exception as e:
            logging.error(f"Error initializing Edge with webdriver-manager: {str(e)}")
            raise e

    # ==================== ç®€åŒ–çš„å¯¼èˆªæ–¹æ³• ====================
    def ensure_navigation(self, force_refresh=False):
        if not self.initialize_driver():
            logging.error("æ­¥éª¤: Failed to initialize driver for navigation.")
            st.error("âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°è¾“å‡º")
            return False
        
        try:
            logging.debug("æ­¥éª¤: Ensuring navigation...")
            target_url = "https://www.iwencai.com/unifiedwap/"
            
            if force_refresh:
                logging.debug(f"æ­¥éª¤: Force refreshing to {target_url}")
                self.driver.get(target_url)
            else:
                current_url = self.driver.current_url
                if target_url not in current_url:
                    logging.debug(f"æ­¥éª¤: Navigating to {target_url}")
                    self.driver.get(target_url)
            
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            logging.debug("æ­¥éª¤: Navigation successful.")
            return True
            
        except Exception as e:
            logging.error(f"Error in navigation: {str(e)}")
            st.error(f"âŒ å¯¼èˆªå¤±è´¥: {str(e)}")
            return False

    # ==================== ç®€åŒ–çš„ç™»å½•å¤„ç† ====================
    def handle_login_smartly(self):
        """ç®€åŒ–çš„ç™»å½•å¤„ç†"""
        try:
            logging.debug("æ­¥éª¤: Checking for login requirement...")
            
            login_indicators = [
                "//div[contains(text(), 'æ‰«ç ç™»å½•')]",
                "//div[contains(@class, 'login')]",
                "//div[contains(@class, 'qrcode')]",
            ]
            
            for selector in login_indicators:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    for element in elements:
                        if element.is_displayed():
                            logging.debug(f"æ­¥éª¤: Login popup detected with: {selector}")
                            return self.wait_for_login_completion()
                except:
                    continue
            
            logging.debug("æ­¥éª¤: No login required.")
            return True
            
        except Exception as e:
            logging.error(f"Error in login handling: {str(e)}")
            return False

    def wait_for_login_completion(self, timeout=120):
        """ç­‰å¾…ç™»å½•å®Œæˆ"""
        logging.debug("æ­¥éª¤: Waiting for login completion...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            login_visible = False
            try:
                login_elements = self.driver.find_elements(By.XPATH, "//div[contains(text(), 'æ‰«ç ç™»å½•')]")
                for element in login_elements:
                    if element.is_displayed():
                        login_visible = True
                        break
            except:
                pass
            
            if not login_visible:
                self.is_logged_in = True
                logging.debug("æ­¥éª¤: Login completed successfully.")
                time.sleep(2)
                return True
            
            time.sleep(2)
        
        logging.warning("æ­¥éª¤: Login timeout.")
        return False

    # ==================== æ”¹è¿›çš„ä¸‹è½½æµç¨‹ ====================
    def smart_download_flow_optimized(self):
        """æ”¹è¿›çš„ä¸‹è½½æµç¨‹"""
        try:
            logging.debug("æ­¥éª¤: Starting optimized download flow...")
            
            download_start_time = time.time()
            
            self.clean_download_directory()
            
            btn = self.find_and_cache_download_button()
            if not btn:
                logging.error("æ­¥éª¤: Download button not found.")
                btn = self.find_alternative_download_button()
                if not btn:
                    return False
            
            logging.debug("æ­¥éª¤: Clicking download button...")
            try:
                self.driver.execute_script("arguments[0].scrollIntoView(true);", btn)
                time.sleep(1)
                self.driver.execute_script("arguments[0].click();", btn)
            except Exception as e:
                logging.error(f"JavaScript click failed: {str(e)}")
                try:
                    btn.click()
                except Exception as e2:
                    logging.error(f"Regular click also failed: {str(e2)}")
                    return False
            
            time.sleep(3)
            if not self.is_logged_in:
                self.handle_login_smartly()
            
            if self.is_logged_in:
                time.sleep(3)
                btn = self.find_and_cache_download_button()
                if btn:
                    try:
                        self.driver.execute_script("arguments[0].click();", btn)
                    except:
                        btn.click()
            
            return self.wait_for_download_complete_fast(download_start_time, timeout=60)
            
        except Exception as e:
            logging.error(f"Error in download flow: {str(e)}")
            return False

    def clean_download_directory(self):
        """æ¸…ç©ºä¸‹è½½ç›®å½•"""
        try:
            files = os.listdir(self.download_dir)
            for file in files:
                file_path = os.path.join(self.download_dir, file)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        logging.debug(f"æ­¥éª¤: Removed old file: {file}")
                except Exception as e:
                    logging.warning(f"Could not remove file {file}: {str(e)}")
        except Exception as e:
            logging.error(f"Error cleaning download directory: {str(e)}")

    def find_alternative_download_button(self):
        """å°è¯•å…¶ä»–ä¸‹è½½æŒ‰é’®é€‰æ‹©å™¨"""
        alternative_selectors = [
            "//*[contains(@class, 'download')]",
            "//*[contains(text(), 'å¯¼å‡º')]",
            "//*[contains(text(), 'ä¸‹è½½')]",
            "//button[contains(@class, 'btn-download')]",
            "//a[contains(@class, 'download')]",
            "//span[contains(text(), 'å¯¼å‡º')]",
            "//span[contains(text(), 'ä¸‹è½½')]",
        ]
        
        for sel in alternative_selectors:
            try:
                elements = self.driver.find_elements(By.XPATH, sel)
                for element in elements:
                    if element.is_displayed() and element.is_enabled():
                        text = element.text or 'æ— æ–‡æœ¬'
                        logging.debug(f"æ­¥éª¤: Alternative download button found: {sel} - {text}")
                        return element
            except:
                continue
        
        logging.warning("æ­¥éª¤: No alternative download button found.")
        return None

    def wait_for_download_complete_fast(self, start_time, timeout=60):
        """æ”¹è¿›çš„ä¸‹è½½ç­‰å¾…æ–¹æ³•ï¼ŒåŸºäºæ—¶é—´æˆ³æ£€æµ‹æ–°æ–‡ä»¶"""
        try:
            logging.debug("æ­¥éª¤: Waiting for download...")
            temp_extensions = ['.crdownload', '.part', '.tmp', '.temp']
            
            wait_start_time = time.time()
            
            while time.time() - wait_start_time < timeout:
                try:
                    files = os.listdir(self.download_dir)
                    logging.debug(f"æ­¥éª¤: Current files in directory: {files}")
                    
                    for file in files:
                        file_path = os.path.join(self.download_dir, file)
                        
                        if any(file.endswith(ext) for ext in temp_extensions):
                            logging.debug(f"æ­¥éª¤: Skipping temp file: {file}")
                            continue
                            
                        if os.path.getsize(file_path) > 0:
                            mtime = os.path.getmtime(file_path)
                            ctime = os.path.getctime(file_path)
                            file_time = max(mtime, ctime)
                            
                            if file_time >= start_time:
                                logging.debug(f"æ­¥éª¤: Download completed with file: {file}")
                                logging.debug(f"æ­¥éª¤: File time: {file_time}, Start time: {start_time}")
                                return True
                            
                            file_size = os.path.getsize(file_path)
                            logging.debug(f"æ­¥éª¤: File {file} - Size: {file_size}, Time: {file_time}")
                except Exception as e:
                    logging.error(f"Error checking download directory: {str(e)}")
                
                time.sleep(2)
            
            files = os.listdir(self.download_dir)
            if files:
                logging.warning(f"æ­¥éª¤: Timeout but found files: {files}")
                for file in files:
                    file_path = os.path.join(self.download_dir, file)
                    if os.path.getsize(file_path) > 0:
                        logging.warning(f"æ­¥éª¤: Using existing file: {file}")
                        return True
                
            logging.warning("æ­¥éª¤: Download timeout.")
            return False
            
        except Exception as e:
            logging.error(f"Error waiting for download: {str(e)}")
            return False

    def find_and_cache_download_button(self):
        logging.debug("æ­¥éª¤: Searching for download button...")
        selectors = [
            "//div[contains(@class, 'item')]//div[contains(@class, 'download')]/../div[contains(@class, 'text') and text()='å¯¼æ•°æ®']",
            "//div[contains(@class, 'text') and text()='å¯¼æ•°æ®']",
            "//div[contains(@class, 'item')]//div[contains(@class, 'download')]",
            "//button[contains(text(), 'å¯¼æ•°æ®')]",
            "//span[contains(text(), 'å¯¼æ•°æ®')]",
            "//a[contains(text(), 'å¯¼æ•°æ®')]",
            "//div[contains(text(), 'å¯¼æ•°æ®')]",
        ]
        for sel in selectors:
            try:
                elements = self.driver.find_elements(By.XPATH, sel)
                for element in elements:
                    if element.is_displayed() and element.is_enabled():
                        text = element.text or 'æ— æ–‡æœ¬'
                        self.save_selector_to_cache('download_button', sel, f"ä¸‹è½½æŒ‰é’® - {text}")
                        logging.debug(f"æ­¥éª¤: Download button found: {sel}")
                        return element
            except:
                continue
        logging.warning("æ­¥éª¤: No download button found.")
        return None

    def save_selector_to_cache(self, element_type, selector, description=""):
        self.cached_selectors[element_type] = {
            'selector': selector,
            'description': description,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    # ==================== ä¸€é”®è‡ªåŠ¨åŒ– ====================
    def one_click_automation_with_refresh(self, search_query):
        try:
            logging.debug("æ­¥éª¤: Starting automation...")
            
            if not self.ensure_navigation(force_refresh=True):
                return False
            time.sleep(3)
            
            if not self.find_search_box_with_cache(search_query):
                return False
            
            if not self.find_search_button_with_cache():
                return False
            time.sleep(5)
            
            if not self.smart_download_flow_optimized():
                return False
            
            logging.debug("æ­¥éª¤: Automation completed successfully.")
            return True
            
        except Exception as e:
            logging.error(f"Error in automation: {str(e)}")
            return False

    def find_search_box_with_cache(self, search_query):
        try:
            logging.debug(f"æ­¥éª¤: Filling search box: {search_query}")
            sel = self.cached_selectors['search_box']['selector']
            el = self.driver.find_element(By.XPATH, sel)
            if el.is_displayed() and el.is_enabled():
                el.click()
                time.sleep(0.5)
                el.clear()
                time.sleep(0.5)
                el.send_keys(search_query)
                logging.debug("æ­¥éª¤: Search box filled.")
                return True
        except Exception as e:
            logging.error(f"Error with search box: {str(e)}")
        return False

    def find_search_button_with_cache(self):
        try:
            logging.debug("æ­¥éª¤: Clicking search button...")
            sel = self.cached_selectors['search_button']['selector']
            el = self.driver.find_element(By.XPATH, sel)
            if el.is_displayed() and el.is_enabled():
                el.click()
                time.sleep(3)
                logging.debug("æ­¥éª¤: Search button clicked.")
                return True
        except Exception as e:
            logging.error(f"Error with search button: {str(e)}")
        return False

    # ==================== ä¸“é—¨ä¼˜åŒ–çš„åŒè¡¨å¤´å¤„ç†æ–¹æ³• ====================
    def process_downloaded_data(self):
        try:
            logging.debug("æ­¥éª¤: Processing downloaded data...")
            files = os.listdir(self.download_dir)
            logging.debug(f"æ­¥éª¤: All files in download directory: {files}")
            
            if not files:
                logging.warning("æ­¥éª¤: No files in download directory.")
                return None
            
            latest_file = None
            latest_time = 0
            
            for file in files:
                file_path = os.path.join(self.download_dir, file)
                file_time = os.path.getmtime(file_path)
                if file_time > latest_time:
                    latest_time = file_time
                    latest_file = file
            
            if not latest_file:
                logging.warning("æ­¥éª¤: Could not determine latest file.")
                return None
                
            file_path = os.path.join(self.download_dir, latest_file)
            logging.debug(f"æ­¥éª¤: Processing latest file: {latest_file}")
            
            if latest_file.endswith('.csv'):
                df = self.read_iwencai_csv_improved(file_path)
            elif latest_file.endswith(('.xls', '.xlsx')):
                df = self.read_iwencai_excel_improved(file_path)
            else:
                df = self.auto_detect_iwencai_file_improved(file_path)
                
            if df is None or df.empty:
                logging.warning("æ­¥éª¤: Dataframe is empty or could not be read.")
                return None
                
            stock_count = len(df)
            slope_data, closing_sequences, date_columns, stock_names = self.calculate_slopes_improved(df)
            
            # è®¡ç®—æ–°å‡ºç°çš„è‚¡ç¥¨
            new_stocks = self.calculate_new_stocks(df)
            
            logging.debug(f"æ­¥éª¤: Successfully processed {stock_count} stocks")
            logging.debug(f"æ­¥éª¤: New stocks detected: {len(new_stocks)}")
            
            return {
                'timestamp': datetime.now(),
                'stock_count': stock_count,
                'stock_list': df,
                'slopes': slope_data,
                'closing_sequences': closing_sequences,
                'date_columns': date_columns,
                'stock_names': stock_names,
                'new_stocks': new_stocks
            }
        except Exception as e:
            logging.error(f"Error processing data: {str(e)}")
            return None

    def calculate_new_stocks(self, current_df):
        """è®¡ç®—æ–°å‡ºç°çš„è‚¡ç¥¨"""
        new_stocks = []
        
        # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œæ‰€æœ‰è‚¡ç¥¨éƒ½æ˜¯æ–°çš„
        if not self.monitoring_data['stock_lists']:
            for index, row in current_df.iterrows():
                stock_code = self.get_stock_code(row, current_df.columns)
                stock_name = self.get_stock_name(row, current_df.columns)
                new_stocks.append(f"{stock_code} {stock_name}".strip())
            return new_stocks
        
        # è·å–ä¸Šä¸€æ¬¡çš„è‚¡ç¥¨åˆ—è¡¨
        last_df = self.monitoring_data['stock_lists'][-1]
        
        # è·å–å½“å‰å’Œä¸Šä¸€æ¬¡çš„è‚¡ç¥¨ä»£ç é›†åˆ
        current_stocks = set()
        for index, row in current_df.iterrows():
            stock_code = self.get_stock_code(row, current_df.columns)
            stock_name = self.get_stock_name(row, current_df.columns)
            current_stocks.add(f"{stock_code} {stock_name}".strip())
        
        last_stocks = set()
        for index, row in last_df.iterrows():
            stock_code = self.get_stock_code(row, last_df.columns)
            stock_name = self.get_stock_name(row, last_df.columns)
            last_stocks.add(f"{stock_code} {stock_name}".strip())
        
        # è®¡ç®—æ–°å‡ºç°çš„è‚¡ç¥¨
        new_stocks = list(current_stocks - last_stocks)
        
        return new_stocks

    def read_iwencai_excel_improved(self, file_path):
        """ä¸“é—¨ä¼˜åŒ–åŒè¡¨å¤´å¤„ç†çš„Excelè¯»å–æ–¹æ³• - å‚è€ƒä¸Šä¼ æ–‡ä»¶å¤„ç†ä»£ç """
        try:
            # å…ˆè¯»å–å‰å‡ è¡Œæ¥æ£€æµ‹è¡¨å¤´ç»“æ„
            df_raw = pd.read_excel(file_path, header=None, nrows=10)
            logging.debug("æ­¥éª¤: Raw Excel data preview:")
            for i in range(min(10, len(df_raw))):
                logging.debug(f"Row {i}: {df_raw.iloc[i].tolist()}")
            
            # æ£€æµ‹è¡¨å¤´è¡Œæ•°
            header_rows = self.detect_header_rows_improved(df_raw)
            logging.debug(f"æ­¥éª¤: Detected header rows: {header_rows}")
            
            if header_rows == 1:
                # å•è¡¨å¤´æƒ…å†µ
                df = pd.read_excel(file_path, header=0)
                df.columns = [str(c).strip() for c in df.columns]
            else:
                # å¤šè¡Œè¡¨å¤´æƒ…å†µ - ä½¿ç”¨ä¸Šä¼ æ–‡ä»¶å¤„ç†ä»£ç çš„æ–¹æ³•
                df = self.process_double_header_excel_improved(file_path, header_rows)
            
            df = self.basic_data_cleaning(df)
            
            logging.debug(f"æ­¥éª¤: Final columns after processing: {list(df.columns)}")
            return df
            
        except Exception as e:
            logging.error(f"Error reading improved Excel: {str(e)}")
            return pd.read_excel(file_path)

    def detect_header_rows_improved(self, df_preview):
        """æ”¹è¿›çš„è¡¨å¤´è¡Œæ•°æ£€æµ‹ - å‚è€ƒä¸Šä¼ æ–‡ä»¶å¤„ç†ä»£ç """
        header_keywords = ['ä»£ç ', 'åç§°', 'æ”¶ç›˜ä»·', 'å¼€ç›˜ä»·', '5æ—¥å‡çº¿', 'å‡çº¿', 'è´¢åŠ¡è¯Šæ–­è¯„åˆ†', 'undefined']
        
        for i in range(min(5, len(df_preview))):
            row_text = ' '.join([str(x) for x in df_preview.iloc[i] if pd.notna(x)])
            if any(keyword in row_text for keyword in header_keywords):
                if i == 0:
                    # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦åŒ…å«æ—¥æœŸæˆ–æŠ€æœ¯æŒ‡æ ‡
                    if len(df_preview) > 1:
                        next_row_text = ' '.join([str(x) for x in df_preview.iloc[1] if pd.notna(x)])
                        if self.contains_date_or_technical_improved(next_row_text):
                            return 2
                    return 1
                else:
                    return i + 1
        
        return 1

    def contains_date_or_technical_improved(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ—¥æœŸæˆ–æŠ€æœ¯æŒ‡æ ‡ä¿¡æ¯ - æ”¹è¿›ç‰ˆæœ¬"""
        date_indicators = ['2024', '2025', 'æ”¶ç›˜ä»·', 'å¼€ç›˜ä»·', 'å‡çº¿', 'MA', 'undefined', 'å‰', 'å']
        text_str = str(text).lower()
        return any(indicator in text_str for indicator in date_indicators)

    def process_double_header_excel_improved(self, file_path, header_rows):
        """å¤„ç†åŒè¡¨å¤´ - å‚è€ƒä¸Šä¼ æ–‡ä»¶å¤„ç†ä»£ç çš„æ–¹æ³•"""
        try:
            # è¯»å–åŸå§‹æ•°æ®
            df_raw = pd.read_excel(file_path, header=None)
            
            # å¤„ç†è¡¨å¤´è¡Œï¼Œå‘å‰å¡«å……ç©ºå€¼
            header_df = df_raw.iloc[:header_rows].ffill(axis=1)
            df = df_raw.iloc[header_rows:].reset_index(drop=True)
            
            # æ„å»ºåˆå¹¶åˆ—å - å‚è€ƒä¸Šä¼ æ–‡ä»¶å¤„ç†ä»£ç 
            columns = []
            current_prefix = ""
            
            for col in header_df.values.T:
                col_strs = [str(x).strip() for x in col if str(x) != "nan"]
                if len(col_strs) == 0:
                    columns.append("")
                    continue
                    
                # è¯†åˆ«åˆ—ç±»å‹å‰ç¼€
                if "æ”¶ç›˜ä»·" in col_strs[0]:
                    current_prefix = "æ”¶ç›˜ä»·"
                elif "5æ—¥å‡çº¿" in col_strs[0] or "å‡çº¿" in col_strs[0]:
                    current_prefix = "5æ—¥å‡çº¿"
                elif "å¼€ç›˜ä»·" in col_strs[0]:
                    current_prefix = "å¼€ç›˜ä»·"
                elif "è´¢åŠ¡è¯Šæ–­è¯„åˆ†" in col_strs[0]:
                    current_prefix = "è´¢åŠ¡è¯Šæ–­è¯„åˆ†"
                
                # æå–æ—¥æœŸéƒ¨åˆ†
                date_part = col_strs[-1] if len(col_strs) > 1 else col_strs[0]
                
                # æ„å»ºåˆ—å
                if current_prefix and "undefined" in col_strs[0]:
                    merged = f"{current_prefix}_{date_part}"
                else:
                    merged = "_".join(col_strs).strip("_")
                
                columns.append(merged)
            
            df.columns = columns
            return df
            
        except Exception as e:
            logging.error(f"Error processing double header improved: {str(e)}")
            return pd.read_excel(file_path, header=1)

    def basic_data_cleaning(self, df):
        """åŸºç¡€æ•°æ®æ¸…æ´—"""
        if df is None or df.empty:
            return df
        
        df_clean = df.copy()
        
        for col in df_clean.select_dtypes(include=['object']).columns:
            try:
                df_clean[col] = df_clean[col].astype(str).str.strip().replace({
                    'nan': np.nan, 'None': np.nan, '': np.nan
                })
            except Exception:
                pass
        
        replace_symbols = ["-", "â€”", "ç©ºå€¼", "null", "None", "", "NaN", "--"]
        df_clean.replace(replace_symbols, np.nan, inplace=True)
        
        for col in df_clean.columns:
            if df_clean[col].dtype == object:
                try:
                    df_clean[col] = df_clean[col].astype(str).str.replace(',', '').str.replace(' ', '')
                except Exception:
                    pass
                try:
                    df_clean[col] = pd.to_numeric(df_clean[col], errors='ignore')
                except Exception:
                    pass
        
        df_clean = df_clean.dropna(how='all')
        df_clean = df_clean.dropna(axis=1, how='all')
        
        df_clean = self.identify_stock_columns(df_clean)
        
        return df_clean

    def find_closing_price_columns(self, df):
        """æŸ¥æ‰¾æ”¶ç›˜ä»·åˆ— - æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒºåˆ†æ”¶ç›˜ä»·ã€å¼€ç›˜ä»·å’Œ5æ—¥å‡çº¿"""
        close_cols = []
        date_info = []
        
        for col in df.columns:
            col_str = str(col)
            
            # åªè¯†åˆ«æ˜ç¡®æ ‡è®°ä¸ºæ”¶ç›˜ä»·çš„åˆ—
            is_closing_col = False
            if col_str.startswith('æ”¶ç›˜ä»·_'):
                is_closing_col = True
            elif 'æ”¶ç›˜ä»·' in col_str and 'å¼€ç›˜ä»·' not in col_str and '5æ—¥å‡çº¿' not in col_str:
                is_closing_col = True
            
            if is_closing_col:
                # ä»åˆ—åä¸­æå–æ—¥æœŸ
                parts = str(col).split('_')
                if len(parts) > 1:
                    date_str_raw = parts[-1]
                    date_str = date_str_raw.split(' [')[0].strip()
                    
                    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼è§£æ
                    date_obj = None
                    for fmt in ("%Y.%m.%d", "%Y-%m-%d", "%Y%m%d", "%Y/%m/%d"):
                        try:
                            date_obj = datetime.strptime(date_str, fmt)
                            break
                        except:
                            continue
                    
                    if date_obj:
                        extracted_date = date_obj.strftime("%Y-%m-%d")
                    else:
                        # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
                        extracted_date = date_str
                else:
                    extracted_date = col_str
                
                if self.is_valid_price_column(df[col]):
                    close_cols.append(col)
                    date_info.append(extracted_date)
        
        logging.debug(f"æ­¥éª¤: Closing price columns found: {close_cols}")
        logging.debug(f"æ­¥éª¤: Corresponding dates: {date_info}")
        
        if close_cols and date_info:
            close_cols, date_info = self.sort_columns_by_date(close_cols, date_info)
            logging.debug(f"æ­¥éª¤: Sorted closing price columns: {close_cols}")
            logging.debug(f"æ­¥éª¤: Sorted dates: {date_info}")
        
        return close_cols, date_info

    def is_valid_price_column(self, series):
        """æ£€æŸ¥åˆ—æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä»·æ ¼æ•°æ®"""
        if series.empty:
            return False
        
        if not pd.api.types.is_numeric_dtype(series):
            try:
                series_numeric = pd.to_numeric(series, errors='coerce')
                if series_numeric.isna().all():
                    return False
            except:
                return False
        
        numeric_series = pd.to_numeric(series, errors='coerce')
        valid_values = numeric_series.dropna()
        if len(valid_values) == 0:
            return False
        
        avg_value = valid_values.mean()
        return 0.1 <= avg_value <= 10000

    def sort_columns_by_date(self, columns, dates):
        """æŒ‰æ—¥æœŸå¯¹åˆ—è¿›è¡Œæ’åº"""
        pairs = list(zip(columns, dates))
        
        def parse_date(date_str):
            try:
                # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                for fmt in ("%Y-%m-%d", "%Y.%m.%d", "%Y%m%d", "%Y/%m/%d"):
                    try:
                        return datetime.strptime(date_str, fmt)
                    except:
                        continue
                return datetime(1900, 1, 1)
            except:
                return datetime(1900, 1, 1)
        
        sorted_pairs = sorted(pairs, key=lambda x: parse_date(x[1]))
        
        sorted_columns = [pair[0] for pair in sorted_pairs]
        sorted_dates = [pair[1] for pair in sorted_pairs]
        
        return sorted_columns, sorted_dates

    def calculate_slopes_improved(self, df):
        """æ”¹è¿›çš„æ–œç‡è®¡ç®—æ–¹æ³• - ä½¿ç”¨7å¤©æ•°æ®"""
        slopes = {}
        closing_sequences = {}
        date_columns_info = {}
        stock_names = {}
        
        close_cols, date_info = self.find_closing_price_columns(df)
        logging.debug(f"æ­¥éª¤: Found {len(close_cols)} closing price columns: {close_cols}")
        logging.debug(f"æ­¥éª¤: Date info: {date_info}")
        
        if len(close_cols) < 2:
            logging.warning(f"æ­¥éª¤: Not enough closing price columns found. Need at least 2, found {len(close_cols)}")
            for index, row in df.iterrows():
                stock_code = self.get_stock_code(row, df.columns)
                stock_name = self.get_stock_name(row, df.columns)
                key = f"{stock_code} {stock_name}".strip()
                slopes[key] = 0
                closing_sequences[key] = []
                date_columns_info[key] = []
                stock_names[key] = stock_name
            return slopes, closing_sequences, date_columns_info, stock_names
        
        # åªå–æœ€è¿‘çš„7å¤©æ•°æ®
        if len(close_cols) > 7:
            close_cols = close_cols[-7:]
            date_info = date_info[-7:]
            logging.debug(f"æ­¥éª¤: Using last 7 days data: {close_cols}")
            logging.debug(f"æ­¥éª¤: Corresponding dates: {date_info}")
        
        for index, row in df.iterrows():
            stock_code = self.get_stock_code(row, df.columns)
            stock_name = self.get_stock_name(row, df.columns)
            
            closes = []
            valid_dates = []
            
            for i, col in enumerate(close_cols):
                val = row.get(col, np.nan)
                if pd.notna(val):
                    val_str = str(val).replace(',', '').replace('â€”', '').replace('--', '').strip()
                    if val_str in ["", "NaN", "None", "null"]:
                        continue
                    try:
                        price = float(val_str)
                        if price > 0:
                            closes.append(price)
                            valid_dates.append(date_info[i])
                    except Exception as e:
                        logging.debug(f"æ­¥éª¤: Failed to convert value '{val_str}' to float for column {col}: {str(e)}")
                        continue
            
            logging.debug(f"æ­¥éª¤: Stock {stock_code} {stock_name} - Valid dates: {valid_dates}")
            logging.debug(f"æ­¥éª¤: Stock {stock_code} {stock_name} - Raw prices: {closes}")
            
            key = f"{stock_code} {stock_name}".strip()
            closing_sequences[key] = closes
            date_columns_info[key] = valid_dates
            stock_names[key] = stock_name
            
            if len(closes) < 2:
                logging.debug(f"æ­¥éª¤: Insufficient price data for {stock_code} {stock_name}, only {len(closes)} valid values")
                slopes[key] = 0
                continue
            
            try:
                x = np.arange(len(closes))
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, closes)
                
                avg_price = np.mean(closes)
                slope_percentage = (slope / avg_price) * 100 if avg_price != 0 else 0
                
                slopes[key] = slope_percentage
                logging.debug(f"æ­¥éª¤: Calculated slope for {key}: {slope_percentage:.4f}% (slope={slope:.4f}, avg_price={avg_price:.4f})")
                
            except Exception as e:
                logging.warning(f"æ­¥éª¤: Failed to calculate slope for {stock_code} {stock_name}: {str(e)}")
                slopes[key] = 0
    
        return slopes, closing_sequences, date_columns_info, stock_names

    def read_iwencai_csv_improved(self, file_path):
        """æ”¹è¿›çš„CSVè¯»å–æ–¹æ³•"""
        try:
            encodings = ['gbk', 'utf-8', 'gb2312', 'utf-8-sig']
            
            for encoding in encodings:
                try:
                    df_raw = pd.read_csv(file_path, encoding=encoding, header=None, nrows=10)
                    
                    header_rows = self.detect_header_rows_improved(df_raw)
                    
                    if header_rows == 1:
                        df = pd.read_csv(file_path, encoding=encoding, header=0)
                    else:
                        df = self.process_double_header_csv_improved(file_path, encoding, header_rows)
                    
                    df = self.basic_data_cleaning(df)
                    return df
                    
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    logging.debug(f"Failed to read CSV with encoding {encoding}: {str(e)}")
                    continue
            
            return pd.read_csv(file_path)
            
        except Exception as e:
            logging.error(f"Error reading improved CSV: {str(e)}")
            return None

    def process_double_header_csv_improved(self, file_path, encoding, header_rows):
        """å¤„ç†CSVçš„åŒè¡¨å¤´ - æ”¹è¿›ç‰ˆæœ¬"""
        try:
            df_raw = pd.read_csv(file_path, encoding=encoding, header=None)
            header_df = df_raw.iloc[:header_rows].ffill(axis=1)
            df = df_raw.iloc[header_rows:].reset_index(drop=True)
            
            columns = []
            current_prefix = ""
            
            for col in header_df.values.T:
                col_strs = [str(x).strip() for x in col if str(x) != "nan"]
                if len(col_strs) == 0:
                    columns.append("")
                    continue
                    
                if "æ”¶ç›˜ä»·" in col_strs[0]:
                    current_prefix = "æ”¶ç›˜ä»·"
                elif "5æ—¥å‡çº¿" in col_strs[0] or "å‡çº¿" in col_strs[0]:
                    current_prefix = "5æ—¥å‡çº¿"
                elif "å¼€ç›˜ä»·" in col_strs[0]:
                    current_prefix = "å¼€ç›˜ä»·"
                elif "è´¢åŠ¡è¯Šæ–­è¯„åˆ†" in col_strs[0]:
                    current_prefix = "è´¢åŠ¡è¯Šæ–­è¯„åˆ†"
                
                date_part = col_strs[-1] if len(col_strs) > 1 else col_strs[0]
                
                if current_prefix and "undefined" in col_strs[0]:
                    merged = f"{current_prefix}_{date_part}"
                else:
                    merged = "_".join(col_strs).strip("_")
                
                columns.append(merged)
            
            df.columns = columns
            return df
            
        except Exception as e:
            logging.error(f"Error processing double header CSV improved: {str(e)}")
            return pd.read_csv(file_path, encoding=encoding, header=1)

    def auto_detect_iwencai_file_improved(self, file_path):
        """æ”¹è¿›çš„è‡ªåŠ¨æ–‡ä»¶æ£€æµ‹"""
        try:
            df = self.read_iwencai_excel_improved(file_path)
            if df is not None and not df.empty:
                return df
            
            df = self.read_iwencai_csv_improved(file_path)
            if df is not None and not df.empty:
                return df
                
            return None
        except Exception as e:
            logging.error(f"Auto detect improved failed: {str(e)}")
            return None

    def identify_stock_columns(self, df):
        """è¯†åˆ«è‚¡ç¥¨ä»£ç å’Œåç§°åˆ—"""
        df_clean = df.copy()
        
        code_patterns = ['ä»£ç ', 'code', 'symbol']
        for col in df_clean.columns:
            col_lower = str(col).lower()
            if any(pattern in col_lower for pattern in code_patterns):
                df_clean = df_clean.rename(columns={col: 'è‚¡ç¥¨ä»£ç '})
                break
        
        name_patterns = ['åç§°', 'name', 'è‚¡ç¥¨åç§°', 'è‚¡ç¥¨ç®€ç§°']
        for col in df_clean.columns:
            col_lower = str(col).lower()
            if any(pattern in col_lower for pattern in name_patterns):
                df_clean = df_clean.rename(columns={col: 'è‚¡ç¥¨åç§°'})
                break
        
        return df_clean

    def get_stock_code(self, row, columns):
        code_keywords = ['ä»£ç ', 'code', 'symbol', 'è‚¡ç¥¨ä»£ç ']
        for col in columns:
            if any(keyword in str(col).lower() for keyword in code_keywords):
                return str(row[col]) if pd.notna(row[col]) else f"ä»£ç {row.name}"
        return f"ä»£ç {row.name}"

    def get_stock_name(self, row, columns):
        name_keywords = ['åç§°', 'name', 'è‚¡ç¥¨åç§°', 'è‚¡ç¥¨ç®€ç§°']
        for col in columns:
            if any(keyword in str(col).lower() for keyword in name_keywords):
                return str(row[col]) if pd.notna(row[col]) else f"è‚¡ç¥¨{row.name}"
        return f"è‚¡ç¥¨{row.name}"

    # ==================== ç›‘æ§æ§åˆ¶æ–¹æ³• ====================
    def start_monitoring(self, interval_minutes=5):
        if self.is_monitoring:
            st.warning("ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        self.monitoring_interval = interval_minutes
        self.is_monitoring = True
        self.cycle_count = 1
        st.success(f"ç›‘æ§å¯åŠ¨ï¼Œæ¯{interval_minutes}åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡")
        self.execute_monitoring_cycle(st.session_state.search_query)
        self.next_execution_time = self.last_execution_time + timedelta(minutes=interval_minutes)

    def stop_monitoring(self):
        self.is_monitoring = False
        self.next_execution_time = None
        st.success("ç›‘æ§å·²åœæ­¢")

    def execute_monitoring_cycle(self, search_query):
        try:
            cycle_start = datetime.now()
            self.last_execution_time = cycle_start
            success = self.one_click_automation_with_refresh(search_query)
            if success:
                data = self.process_downloaded_data()
                if data:
                    self.monitoring_data['timestamps'].append(data['timestamp'])
                    self.monitoring_data['stock_counts'].append(data['stock_count'])
                    self.monitoring_data['stock_lists'].append(data['stock_list'])
                    self.monitoring_data['slope_data'].append(data['slopes'])
                    self.monitoring_data['closing_sequences'].append(data['closing_sequences'])
                    self.monitoring_data['date_columns'].append(data['date_columns'])
                    self.monitoring_data['stock_names'].append(data['stock_names'])
                    self.monitoring_data['new_stocks'].append(data['new_stocks'])
                    return True
            return False
        except Exception as e:
            logging.error(f"Error in monitoring cycle: {str(e)}")
            return False

    def update_countdown(self):
        if self.next_execution_time and self.is_monitoring:
            now = datetime.now()
            if now < self.next_execution_time:
                self.countdown_seconds = int((self.next_execution_time - now).total_seconds())
                return True
        self.countdown_seconds = 0
        return False

    def get_countdown_display(self):
        if self.countdown_seconds > 0:
            m = self.countdown_seconds // 60
            s = self.countdown_seconds % 60
            return f"{m:02d}:{s:02d}"
        return "00:00"

    def create_stock_count_chart(self):
        if len(self.monitoring_data['timestamps']) < 1:
            st.info("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œä¸€é”®è‡ªåŠ¨åŒ–æµ‹è¯•")
            return
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=self.monitoring_data['timestamps'],
            y=self.monitoring_data['stock_counts'],
            mode='lines+markers',
            name='è‚¡ç¥¨æ•°é‡',
            line=dict(color='blue', width=2),
            marker=dict(size=8)
        ))
        fig.update_layout(
            title='è‚¡ç¥¨æ•°é‡æ—¶é—´åºåˆ—',
            xaxis_title='æ—¶é—´',
            yaxis_title='è‚¡ç¥¨æ•°é‡',
            template='plotly_white',
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)

    def create_slope_chart(self):
        if not self.monitoring_data['slope_data']:
            st.info("æš‚æ— æ–œç‡æ•°æ®")
            return
        latest_slopes = self.monitoring_data['slope_data'][-1]
        if not latest_slopes:
            return
        sorted_slopes = sorted(latest_slopes.items(), key=lambda x: x[1], reverse=True)
        top_stocks = sorted_slopes[:20]
        if not top_stocks:
            return
        stocks = [item[0] for item in top_stocks]
        slopes = [item[1] for item in top_stocks]
        colors = ['red' if s < 0 else 'green' for s in slopes]
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=slopes,
            y=stocks,
            orientation='h',
            marker_color=colors,
            text=[f"{s:.2f}%" for s in slopes],
            textposition='auto'
        ))
        fig.update_layout(
            title='è‚¡ç¥¨èµ°åŠ¿æ–œç‡æ’åºï¼ˆå‰20åï¼‰- 7å¤©æ–œç‡',
            xaxis_title='æ–œç‡(%)',
            yaxis_title='è‚¡ç¥¨',
            template='plotly_white',
            height=500
        )
        st.plotly_chart(fig, use_container_width=True)

    def create_individual_stock_trend_charts(self):
        """ä¸ºæ¯ä¸ªè‚¡ç¥¨åˆ›å»ºå•ç‹¬çš„èµ°åŠ¿å›¾ - ä½¿ç”¨æ”¹è¿›çš„æ—¥æœŸå¤„ç†"""
        if (not self.monitoring_data['slope_data'] or 
            not self.monitoring_data['closing_sequences'] or
            not self.monitoring_data['date_columns'] or
            not self.monitoring_data['stock_names'] or
            not self.monitoring_data['new_stocks']):
            st.info("æš‚æ— èµ°åŠ¿æ•°æ®")
            return
        
        latest_slopes = self.monitoring_data['slope_data'][-1]
        latest_sequences = self.monitoring_data['closing_sequences'][-1]
        latest_dates = self.monitoring_data['date_columns'][-1]
        latest_stock_names = self.monitoring_data['stock_names'][-1]
        latest_new_stocks = self.monitoring_data['new_stocks'][-1]
        
        if not latest_slopes or not latest_sequences or not latest_dates or not latest_stock_names or not latest_new_stocks:
            return
        
        sorted_slopes = sorted(latest_slopes.items(), key=lambda x: x[1], reverse=True)
        top_stocks = sorted_slopes[:20]
        
        if not top_stocks:
            return
        
        st.subheader("æ–œç‡å‰20è‚¡ç¥¨èµ°åŠ¿å›¾ - 7å¤©æ•°æ®")
        
        for i, (stock, slope) in enumerate(top_stocks):
            if stock in latest_sequences and stock in latest_dates and stock in latest_stock_names:
                price_sequence = latest_sequences[stock]
                date_sequence = latest_dates[stock]
                stock_name = latest_stock_names[stock]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å¢è‚¡ç¥¨
                is_new_stock = stock in latest_new_stocks
                
                if len(price_sequence) >= 2 and len(date_sequence) == len(price_sequence):
                    # æ”¹è¿›çš„æ—¥æœŸå¤„ç†ï¼šç¡®ä¿æ—¥æœŸæŒ‰æ­£ç¡®é¡ºåºæ’åˆ—ä¸”åªåŒ…å«äº¤æ˜“æ—¥
                    try:
                        # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡è¿›è¡Œæ’åº
                        date_objs = []
                        valid_prices = []
                        
                        for date_str, price in zip(date_sequence, price_sequence):
                            try:
                                # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                                date_obj = None
                                for fmt in ("%Y.%m.%d", "%Y-%m-%d", "%Y%m%d", "%Y/%m/%d"):
                                    try:
                                        date_obj = datetime.strptime(date_str, fmt)
                                        break
                                    except:
                                        continue
                                
                                if date_obj:
                                    # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆæ’é™¤å‘¨å…­å‘¨æ—¥ï¼‰
                                    if date_obj.weekday() < 5:  # 0-4è¡¨ç¤ºå‘¨ä¸€åˆ°å‘¨äº”
                                        date_objs.append(date_obj)
                                        valid_prices.append(price)
                                    else:
                                        logging.debug(f"è·³è¿‡éäº¤æ˜“æ—¥: {date_str}")
                                else:
                                    logging.warning(f"æ— æ³•è§£ææ—¥æœŸ: {date_str}")
                            except Exception as e:
                                logging.warning(f"æ—¥æœŸè§£æé”™è¯¯ {date_str}: {str(e)}")
                                continue
                        
                        # å¦‚æœæˆåŠŸè§£æäº†æ—¥æœŸï¼ŒæŒ‰æ—¥æœŸæ’åº
                        if len(date_objs) > 0:
                            # æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
                            sorted_data = sorted(zip(date_objs, valid_prices))
                            sorted_dates = [date.strftime('%Y-%m-%d') for date, _ in sorted_data]
                            sorted_prices = [price for _, price in sorted_data]
                            
                            # ç¡®ä¿åªæ˜¾ç¤º7ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
                            if len(sorted_dates) > 7:
                                sorted_dates = sorted_dates[-7:]
                                sorted_prices = sorted_prices[-7:]
                            
                            date_sequence = sorted_dates
                            price_sequence = sorted_prices
                            logging.debug(f"æ­¥éª¤: Successfully processed dates for {stock}: {date_sequence}")
                        else:
                            # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡ºåºä½†è®°å½•è­¦å‘Š
                            logging.warning(f"æ­¥éª¤: Date parsing incomplete for {stock}, using original order")
                            st.warning(f"è‚¡ç¥¨ {stock} çš„æ—¥æœŸæ•°æ®ä¸å®Œæ•´ï¼Œå¯èƒ½å½±å“å›¾è¡¨æ˜¾ç¤º")
                    except Exception as e:
                        logging.warning(f"Failed to process dates for {stock}: {str(e)}")
                        # å‡ºé”™æ—¶ä¿æŒåŸå§‹é¡ºåº
                    
                    # åˆ›å»ºæŠ˜çº¿å›¾
                    fig = go.Figure()
                    
                    # ä¸»ä»·æ ¼çº¿
                    fig.add_trace(go.Scatter(
                        x=date_sequence,
                        y=price_sequence,
                        mode='lines+markers+text',
                        name=f"{stock}",
                        line=dict(color='#1f77b4', width=3),
                        marker=dict(size=8, color='#ff7f0e'),
                        text=[f"{price:.2f}" for price in price_sequence],
                        textposition="top center",
                        hovertemplate='<b>%{x}</b><br>æ”¶ç›˜ä»·: %{y:.2f}å…ƒ<extra></extra>'
                    ))
                    
                    # æ·»åŠ è¶‹åŠ¿çº¿
                    if len(price_sequence) >= 2:
                        try:
                            x_numeric = np.arange(len(price_sequence))
                            slope_val, intercept, _, _, _ = stats.linregress(x_numeric, price_sequence)
                            trend_line = intercept + slope_val * x_numeric
                            
                            fig.add_trace(go.Scatter(
                                x=date_sequence,
                                y=trend_line,
                                mode='lines',
                                name='è¶‹åŠ¿çº¿',
                                line=dict(color='red', width=2, dash='dash'),
                                opacity=0.7
                            ))
                        except Exception as e:
                            logging.debug(f"Failed to add trend line for {stock}: {str(e)}")
                    
                    # è®¡ç®—ä»·æ ¼èŒƒå›´ç”¨äºè®¾ç½®Yè½´
                    price_min = min(price_sequence) if price_sequence else 0
                    price_max = max(price_sequence) if price_sequence else 0
                    price_range = price_max - price_min
                    y_padding = price_range * 0.1 if price_range > 0 else (price_min * 0.1 if price_min > 0 else 1)
                    
                    # æ›´æ–°å¸ƒå±€ï¼Œåœ¨æ ‡é¢˜ä¸­åŒ…å«è‚¡ç¥¨ç®€ç§°å’Œæ–°è‚¡ç¥¨æ ‡è®°
                    title = f"<b>{stock}</b> - {stock_name} - 7å¤©æ–œç‡: {slope:.2f}%"
                    if is_new_stock:
                        title += " ğŸ†•"  # æ·»åŠ æ–°è‚¡ç¥¨æ ‡è®°
                    
                    fig.update_layout(
                        title=title,
                        xaxis_title='<b>æ—¥æœŸ</b>',
                        yaxis_title='<b>æ”¶ç›˜ä»·(å…ƒ)</b>',
                        template='plotly_white',
                        height=400,
                        showlegend=True,
                        xaxis=dict(
                            tickangle=45,
                            # ä½¿ç”¨categoryç±»å‹ç¡®ä¿æ­£ç¡®æ˜¾ç¤ºæ—¥æœŸ
                            type='category',
                            # ç¡®ä¿xè½´æŒ‰æ—¶é—´é¡ºåºæ˜¾ç¤º
                            categoryorder='array',
                            categoryarray=date_sequence
                        ),
                        yaxis=dict(
                            range=[price_min - y_padding, price_max + y_padding] if price_sequence else [0, 10]
                        ),
                        hovermode='x unified'
                    )
                    
                    # æ˜¾ç¤ºå›¾è¡¨
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºè‚¡ç¥¨ç»Ÿè®¡æ•°æ®
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        if price_sequence:
                            st.metric("æœ€æ–°ä»·æ ¼", f"{price_sequence[-1]:.2f}å…ƒ")
                        else:
                            st.metric("æœ€æ–°ä»·æ ¼", "N/A")
                    with col2:
                        if is_new_stock:
                            st.metric("è‚¡ç¥¨ç®€ç§°", f"{stock_name} ğŸ†•")  # æ–°è‚¡ç¥¨æ ‡è®°
                        else:
                            st.metric("è‚¡ç¥¨ç®€ç§°", stock_name)
                    with col3:
                        if price_sequence and price_sequence[0] != 0:
                            change_percent = (price_sequence[-1] - price_sequence[0]) / price_sequence[0] * 100
                            st.metric("æ¶¨è·Œå¹…", f"{change_percent:.2f}%")
                        else:
                            st.metric("æ¶¨è·Œå¹…", "N/A")
                    with col4:
                        st.metric("æ•°æ®ç‚¹æ•°", len(price_sequence))
                    
                    # æ˜¾ç¤ºæ—¥æœŸèŒƒå›´ä¿¡æ¯
                    if len(date_sequence) >= 2:
                        st.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {date_sequence[0]} è‡³ {date_sequence[-1]} (å…±{len(date_sequence)}ä¸ªäº¤æ˜“æ—¥)")
                    elif len(date_sequence) == 1:
                        st.info(f"æ•°æ®æ—¶é—´: {date_sequence[0]} (å…±{len(date_sequence)}ä¸ªäº¤æ˜“æ—¥)")
                    else:
                        st.warning("æ— æœ‰æ•ˆäº¤æ˜“æ—¥æ•°æ®")
                    
                    # åœ¨å›¾è¡¨ä¹‹é—´æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
                    if i < len(top_stocks) - 1:
                        st.markdown("---")
                else:
                    st.warning(f"è‚¡ç¥¨ {stock} çš„æ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•ç»˜åˆ¶èµ°åŠ¿å›¾")
            else:
                st.warning(f"è‚¡ç¥¨ {stock} ç¼ºå°‘ä»·æ ¼ã€æ—¥æœŸæˆ–åç§°æ•°æ®")

    def show_monitoring_dashboard(self):
        st.header("ç›‘æ§ä»ªè¡¨æ¿")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç›‘æ§æ•°æ®ç‚¹", len(self.monitoring_data['timestamps']))
        with col2:
            if self.monitoring_data['stock_counts']:
                st.metric("æœ€æ–°è‚¡ç¥¨æ•°é‡", self.monitoring_data['stock_counts'][-1])
            else:
                st.metric("æœ€æ–°è‚¡ç¥¨æ•°é‡", 0)
        with col3:
            if self.monitoring_data['timestamps']:
                st.metric("æœ€åæ›´æ–°æ—¶é—´", self.monitoring_data['timestamps'][-1].strftime("%H:%M:%S"))
            else:
                st.metric("æœ€åæ›´æ–°æ—¶é—´", "æ— æ•°æ®")
        with col4:
            if self.is_monitoring:
                self.update_countdown()
                countdown_display = self.get_countdown_display()
                st.metric("ä¸‹æ¬¡æ‰§è¡Œå€’è®¡æ—¶", countdown_display)
            else:
                st.metric("ç›‘æ§çŠ¶æ€", "å·²åœæ­¢")
        
        # æ˜¾ç¤ºæ–°å‡ºç°è‚¡ç¥¨çš„ä¿¡æ¯
        if self.monitoring_data['new_stocks'] and len(self.monitoring_data['new_stocks']) > 0:
            latest_new_stocks = self.monitoring_data['new_stocks'][-1]
            if latest_new_stocks:
                st.subheader("ğŸ‰ æ–°å‡ºç°è‚¡ç¥¨")
                st.info(f"æœ¬æ¬¡åˆ·æ–°å‘ç°äº† {len(latest_new_stocks)} åªæ–°è‚¡ç¥¨")
                for i, stock in enumerate(latest_new_stocks):
                    st.success(f"{i+1}. {stock}")
        
        self.create_stock_count_chart()
        
        col1, col2 = st.columns(2)
        with col1:
            self.create_slope_chart()
        with col2:
            st.subheader("è‚¡ç¥¨èµ°åŠ¿åˆ†æ")
            st.info("ä¸‹æ–¹å°†æ˜¾ç¤ºæ¯ä¸ªè‚¡ç¥¨çš„è¯¦ç»†èµ°åŠ¿å›¾ï¼ŒåŸºäº7å¤©æ”¶ç›˜ä»·è®¡ç®—æ–œç‡")
            st.info("ğŸ†• æ ‡è®°è¡¨ç¤ºæ–°å‡ºç°çš„è‚¡ç¥¨")
            st.info("ğŸ“ˆ æ—¶é—´è½´å·²æŒ‰æ­£ç¡®çš„æ—¶é—´é¡ºåºæ’åˆ—ï¼Œä¸å«å‘¨å…­å‘¨æ—¥")
        
        self.create_individual_stock_trend_charts()
        
        if self.monitoring_data['stock_lists']:
            st.subheader("æœ€æ–°è‚¡ç¥¨åˆ—è¡¨")
            latest_df = self.monitoring_data['stock_lists'][-1]
            
            # æ ‡è®°æ–°å‡ºç°çš„è‚¡ç¥¨
            if self.monitoring_data['new_stocks'] and len(self.monitoring_data['new_stocks']) > 0:
                latest_new_stocks = self.monitoring_data['new_stocks'][-1]
                
                # åˆ›å»ºæ˜¾ç¤ºç”¨çš„DataFrameï¼Œæ·»åŠ æ–°è‚¡ç¥¨æ ‡è®°
                display_df = latest_df.copy()
                
                # æ·»åŠ æ–°è‚¡ç¥¨æ ‡è®°åˆ—
                display_df['æ˜¯å¦æ–°è‚¡ç¥¨'] = ''
                for index, row in display_df.iterrows():
                    stock_code = self.get_stock_code(row, display_df.columns)
                    stock_name = self.get_stock_name(row, display_df.columns)
                    stock_key = f"{stock_code} {stock_name}".strip()
                    if stock_key in latest_new_stocks:
                        display_df.at[index, 'æ˜¯å¦æ–°è‚¡ç¥¨'] = 'ğŸ†•'
                
                st.dataframe(display_df, use_container_width=True)
            else:
                st.dataframe(latest_df, use_container_width=True)
            
            with st.expander("æ•°æ®ç»Ÿè®¡ä¿¡æ¯"):
                st.write(f"æ€»è‚¡ç¥¨æ•°: {len(latest_df)}")
                st.write(f"æ•°æ®åˆ—æ•°: {len(latest_df.columns)}")
                
                numeric_cols = latest_df.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    st.write("æ•°å€¼åˆ—ç»Ÿè®¡:")
                    st.dataframe(latest_df[numeric_cols].describe(), use_container_width=True)

    def close(self):
        self.stop_monitoring()
        if self.driver:
            self.driver.quit()
        if os.path.exists(self.profile_dir):
            shutil.rmtree(self.profile_dir)

# ====================== æ•°æ®å¯¼å‡ºåŠŸèƒ½ ======================
def add_export_functionality(monitor):
    """æ·»åŠ æ•°æ®å¯¼å‡ºåŠŸèƒ½"""
    st.sidebar.subheader("æ•°æ®å¯¼å‡º")
    
    if monitor.monitoring_data['stock_lists']:
        latest_data = monitor.monitoring_data['stock_lists'][-1]
        
        csv_data = latest_data.to_csv(index=False).encode('utf-8-sig')
        st.sidebar.download_button(
            label="å¯¼å‡ºCSV",
            data=csv_data,
            file_name=f"stock_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
        
        excel_buffer = io.BytesIO()
        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            latest_data.to_excel(writer, index=False, sheet_name='è‚¡ç¥¨æ•°æ®')
        st.sidebar.download_button(
            label="å¯¼å‡ºExcel",
            data=excel_buffer.getvalue(),
            file_name=f"stock_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

# ====================== ä¸»å‡½æ•° ======================
def main():
    if 'monitor' not in st.session_state:
        st.session_state.monitor = StockMonitor()
    if 'search_query' not in st.session_state:
        st.session_state.search_query = "2025å¹´11æœˆ12æ—¥æ”¶ç›˜ä»·å¤§äº5æ—¥å‡çº¿ï¼Œ2025å¹´11æœˆ13æ—¥æ”¶ç›˜ä»·å¤§äº5æ—¥å‡çº¿ï¼Œ2025å¹´11æœˆ14æ—¥æ”¶ç›˜ä»·å¤§äº5æ—¥å‡çº¿ï¼Œ2025å¹´11æœˆ17æ—¥æ”¶ç›˜ä»·å¤§äº5æ—¥å‡çº¿ï¼Œ2025å¹´11æœˆ18æ—¥æ”¶ç›˜ä»·å¤§äº5æ—¥å‡çº¿ï¼Œ2025å¹´11æœˆ19æ—¥æ”¶ç›˜ä»·å¤§äº5æ—¥å‡çº¿ï¼Œ2025å¹´11æœˆ20æ—¥æ”¶ç›˜ä»·å¤§äº5æ—¥å‡çº¿ï¼ŒéSTï¼ŒéåŒ—äº¤æ‰€ï¼Œè´¢åŠ¡ç»¼åˆè¯„åˆ†å¤§äº2.5"  # ä¿®æ”¹ä¸º7ä¸ªäº¤æ˜“æ—¥
    
    st.sidebar.title("æ§åˆ¶é¢æ¿")
    
    st.sidebar.subheader("å›ºåŒ–åŒ¹é…çŠ¶æ€")
    cache_data = []
    for element_type, cache_info in st.session_state.monitor.cached_selectors.items():
        if cache_info:
            cache_data.append({
                'å…ƒç´ ç±»å‹': element_type,
                'é€‰æ‹©å™¨': cache_info['selector'],
                'æè¿°': cache_info['description']
            })
        else:
            cache_data.append({
                'å…ƒç´ ç±»å‹': element_type,
                'é€‰æ‹©å™¨': 'æœªç¼“å­˜',
                'æè¿°': 'æœªç¼“å­˜'
            })
    st.sidebar.dataframe(pd.DataFrame(cache_data), use_container_width=True)
    
    st.sidebar.subheader("æœç´¢è®¾ç½®")
    search_query = st.sidebar.text_area("æœç´¢æŸ¥è¯¢", value=st.session_state.search_query, height=100)
    if search_query != st.session_state.search_query:
        st.session_state.search_query = search_query
        st.sidebar.success("æœç´¢æŸ¥è¯¢å·²æ›´æ–°")
    
    if st.sidebar.button("ä¸€é”®è‡ªåŠ¨åŒ–æµ‹è¯•", type="primary"):
        with st.spinner("æ‰§è¡Œä¸€é”®è‡ªåŠ¨åŒ–æµ‹è¯•..."):
            if st.session_state.monitor.one_click_automation_with_refresh(st.session_state.search_query):
                data = st.session_state.monitor.process_downloaded_data()
                if data:
                    st.session_state.monitor.monitoring_data['timestamps'].append(data['timestamp'])
                    st.session_state.monitor.monitoring_data['stock_counts'].append(data['stock_count'])
                    st.session_state.monitor.monitoring_data['stock_lists'].append(data['stock_list'])
                    st.session_state.monitor.monitoring_data['slope_data'].append(data['slopes'])
                    st.session_state.monitor.monitoring_data['closing_sequences'].append(data['closing_sequences'])
                    st.session_state.monitor.monitoring_data['date_columns'].append(data['date_columns'])
                    st.session_state.monitor.monitoring_data['stock_names'].append(data['stock_names'])
                    st.session_state.monitor.monitoring_data['new_stocks'].append(data['new_stocks'])
                    st.success("ä¸€é”®è‡ªåŠ¨åŒ–æµ‹è¯•æˆåŠŸ")
                else:
                    st.error("æ•°æ®å¤„ç†å¤±è´¥")
            else:
                st.error("ä¸€é”®è‡ªåŠ¨åŒ–æµ‹è¯•å¤±è´¥")
    
    st.sidebar.subheader("è‡ªåŠ¨ç›‘æ§")
    interval = st.sidebar.slider("ç›‘æ§é—´éš”(åˆ†é’Ÿ)", 1, 30, 5)
    col1, col2 = st.sidebar.columns(2)
    with col1:
        if st.button("å¼€å§‹ç›‘æ§", type="primary"):
            if not st.session_state.monitor.is_monitoring:
                st.session_state.monitor.start_monitoring(interval)
            else:
                st.warning("ç›‘æ§å·²åœ¨è¿è¡Œ")
    with col2:
        if st.button("åœæ­¢ç›‘æ§"):
            st.session_state.monitor.stop_monitoring()
    
    if st.session_state.monitor.is_monitoring:
        st.sidebar.success("ç›‘æ§è¿è¡Œä¸­")
        if st.session_state.monitor.next_execution_time:
            st.sidebar.info(f"ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {st.session_state.monitor.next_execution_time.strftime('%H:%M:%S')}")
    else:
        st.sidebar.info("ç›‘æ§å·²åœæ­¢")
    
    add_export_functionality(st.session_state.monitor)
    
    st.session_state.monitor.show_monitoring_dashboard()
    
    with st.expander("ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### ç³»ç»Ÿç‰¹æ€§
        - **7å¤©æ–œç‡è®¡ç®—**: åŸºäºæœ€è¿‘7ä¸ªäº¤æ˜“æ—¥çš„æ”¶ç›˜ä»·è®¡ç®—è‚¡ç¥¨èµ°åŠ¿æ–œç‡
        - **æ–°è‚¡ç¥¨è¯†åˆ«**: æ¯æ¬¡åˆ·æ–°è‡ªåŠ¨è¯†åˆ«æ–°å‡ºç°çš„è‚¡ç¥¨å¹¶æ ‡è®°
        - **åŒè¡¨å¤´ä¼˜åŒ–**: ä¸“é—¨ä¼˜åŒ–åŒèŠ±é¡ºåŒè¡¨å¤´æ ¼å¼ï¼Œè‡ªåŠ¨å¤„ç†undefinedå­—æ®µ
        - **æ™ºèƒ½åˆ—å**: é‡åˆ°undefinedå­—æ®µæ—¶ï¼Œè‡ªåŠ¨ä½¿ç”¨å¦ä¸€è¡Œçš„å€¼æ¥å‘½å
        - **è‚¡ç¥¨ç®€ç§°æ˜¾ç¤º**: åœ¨æŠ˜çº¿å›¾æ ‡é¢˜å’Œç¬¬äºŒåˆ—ä¸­æ˜¾ç¤ºè‚¡ç¥¨ç®€ç§°
        - **æ—¥æœŸåŒ¹é…**: è‡ªåŠ¨åŒ¹é…æ”¶ç›˜ä»·åˆ—ä¸å¯¹åº”æ—¥æœŸï¼Œç¡®ä¿èµ°åŠ¿å›¾æ¨ªåæ ‡æ˜¾ç¤ºæ­£ç¡®æ—¥æœŸ
        - **æ—¶é—´è½´ä¼˜åŒ–**: åæ ‡è½´æŒ‰æ­£ç¡®çš„æ—¶é—´é¡ºåºæ’åˆ—ï¼Œä»¥ä¸€å¤©ä¸ºå•ä½ï¼Œä¸å«å‘¨å…­å‘¨æ—¥
        - **å®æ—¶ç›‘æ§**: å¯è®¾ç½®å®šæ—¶è‡ªåŠ¨æ‰§è¡Œ
        - **æ•°æ®å¯¼å‡º**: æ”¯æŒCSVå’ŒExcelæ ¼å¼å¯¼å‡º
        
        ### 7å¤©æ–œç‡è®¡ç®—
        - ç³»ç»Ÿä¼šè‡ªåŠ¨è·å–æœ€è¿‘7ä¸ªäº¤æ˜“æ—¥çš„æ”¶ç›˜ä»·æ•°æ®
        - ä½¿ç”¨çº¿æ€§å›å½’è®¡ç®—è¿™7å¤©çš„ä»·æ ¼è¶‹åŠ¿æ–œç‡
        - æ–œç‡ä»¥ç™¾åˆ†æ¯”å½¢å¼æ˜¾ç¤ºï¼Œè¡¨ç¤ºä»·æ ¼å˜åŒ–çš„è¶‹åŠ¿å¼ºåº¦
        
        ### æ—¶é—´è½´ä¼˜åŒ–
        - è‡ªåŠ¨è¯†åˆ«å’Œè§£ææ—¥æœŸæ ¼å¼
        - æŒ‰æ—¶é—´å…ˆåé¡ºåºæ­£ç¡®æ’åˆ—åæ ‡è½´
        - ç¡®ä¿æ—¶é—´åºåˆ—æ­£ç¡®æ˜¾ç¤ºï¼Œä¸å«å‘¨å…­å‘¨æ—¥
        - æ˜¾ç¤ºå®Œæ•´çš„æ—¶é—´èŒƒå›´ä¿¡æ¯
        
        ### æ–°è‚¡ç¥¨è¯†åˆ«
        - ç³»ç»Ÿä¼šè‡ªåŠ¨æ¯”è¾ƒå½“å‰å’Œä¸Šä¸€æ¬¡çš„è‚¡ç¥¨åˆ—è¡¨
        - æ–°å‡ºç°çš„è‚¡ç¥¨ä¼šåœ¨å›¾è¡¨æ ‡é¢˜å’Œè‚¡ç¥¨åˆ—è¡¨ä¸­æ ‡è®°ä¸º ğŸ†•
        - åœ¨ç›‘æ§ä»ªè¡¨æ¿é¡¶éƒ¨ä¼šæ˜¾ç¤ºæ–°å‡ºç°è‚¡ç¥¨çš„æ•°é‡å’Œåˆ—è¡¨
        
        ### æ“ä½œæ­¥éª¤
        1. ç‚¹å‡»"ä¸€é”®è‡ªåŠ¨åŒ–æµ‹è¯•"è¿›è¡Œé¦–æ¬¡æµ‹è¯•
        2. è®¾ç½®ç›‘æ§é—´éš”æ—¶é—´
        3. ç‚¹å‡»"å¼€å§‹ç›‘æ§"å¯åŠ¨è‡ªåŠ¨ç›‘æ§
        4. ç³»ç»Ÿä¼šå®šæœŸè‡ªåŠ¨æ‰§è¡Œå¹¶æ›´æ–°æ•°æ®
        5. ä½¿ç”¨ä¾§è¾¹æ çš„æ•°æ®å¯¼å‡ºåŠŸèƒ½ä¸‹è½½æ•°æ®
        """)
    
    st.sidebar.markdown("---")
    if st.sidebar.button("å…³é—­ç³»ç»Ÿ"):
        st.session_state.monitor.close()
        st.sidebar.success("ç³»ç»Ÿå·²å…³é—­")
    
    if st.session_state.monitor.is_monitoring:
        now = datetime.now()
        if now >= st.session_state.monitor.next_execution_time:
            st.session_state.monitor.execute_monitoring_cycle(st.session_state.search_query)
            st.session_state.monitor.cycle_count += 1
            st.session_state.monitor.next_execution_time = datetime.now() + timedelta(minutes=st.session_state.monitor.monitoring_interval)
        st.session_state.monitor.update_countdown()
        time.sleep(1)
        st.rerun()

if __name__ == "__main__":
    main()